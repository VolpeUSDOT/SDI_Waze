---
title: "S3 Bucket integration"
output:
  html_document:
    df_print: paged
---

```{r setup, echo = F}
library(tidyverse)
library(DT)
```

Notes on how to share files with other members of the SDI Waze team:

### Writing to shared bucket

The SDI Waze team shared S3 bucket is at this location:
```
s3://prod-sdc-sdi-911061262852-us-east-1-bucket
```

Curated data 
```

aws s3 ls s3://prod-dot-sdc-curated-911061262852-us-east-1
# for one state:

aws s3 ls s3://prod-dot-sdc-curated-911061262852-us-east-1/waze/version=20171031/content/state=TX/table=alert/projection=redshift/year=2017/
```

Only one month available (December) for TX in older version of curated data

```{r}
cmd = "aws s3 ls s3://prod-dot-sdc-curated-911061262852-us-east-1/waze/version=20171031/content/state=TX/table=alert/projection=redshift/year=2017/"
system(cmd)
```

```{r}
cmd = "aws s3 ls s3://prod-dot-sdc-curated-911061262852-us-east-1/waze/version=20180331/content/state=TX/table=alert/projection=redshift/year=2017/"
system(cmd)
```


```{r curatedavail2017}
# Loop over states to find which ones have month directories:
# can also try previous version, 20171031
avail.mo = vector()
use.states = c(state.abb, c("DC", "CA1", "CA2", "CA3"))
find.months = formatC(1:12, width = 2, flag = "0")
for(i in use.states){
  cmd = paste0('aws s3 ls s3://prod-dot-sdc-curated-911061262852-us-east-1/waze/version=20180331/content/state=', i ,'/table=alert/projection=redshift/year=2017/')
  mo_i <- system(cmd, intern = T)
  avail.mo_i <- substr(mo_i, start =nchar(mo_i[1])-2, stop = nchar(mo_i[1])-1)
  
  avail.mo = rbind(avail.mo, c(i, find.months %in% avail.mo_i))
}

avail.mo <- as.data.frame(avail.mo)
colnames(avail.mo) = c("State", find.months)
DT::datatable(avail.mo) %>% formatStyle(2:13, background = styleEqual('TRUE', 'lightgreen'))
```

Look at the previous version of the database, from 2017-10-31:

```{r curatedavail2017_previous}
# Loop over states to find which ones have month directories:
# can also try previous version, 20171031
avail.mo = vector()
use.states = c(state.abb, c("DC", "CA1", "CA2", "CA3"))
find.months = formatC(1:12, width = 2, flag = "0")
for(i in use.states){
  cmd = paste0('aws s3 ls s3://prod-dot-sdc-curated-911061262852-us-east-1/waze/version=20171031/content/state=', i ,'/table=alert/projection=redshift/year=2017/')
  mo_i <- system(cmd, intern = T)
  avail.mo_i <- substr(mo_i, start =nchar(mo_i[1])-2, stop = nchar(mo_i[1])-1)
  
  avail.mo = rbind(avail.mo, c(i, find.months %in% avail.mo_i))
}

avail.mo <- as.data.frame(avail.mo)
colnames(avail.mo) = c("State", find.months)
datatable(avail.mo) %>% formatStyle(2:13, background = styleEqual('TRUE', 'lightgreen'))
```

Check 2018:

```{r curatedavail2018}
# Loop over states to find which ones have complete data before October:
avail.mo = vector()
use.states = c(state.abb, c("DC", "CA1", "CA2", "CA3"))
find.months = formatC(1:12, width = 2, flag = "0")
for(i in use.states){
  cmd = paste0('aws s3 ls s3://prod-dot-sdc-curated-911061262852-us-east-1/waze/version=20180331/content/state=', i ,'/table=alert/projection=redshift/year=2018/')
  mo_i <- system(cmd, intern = T)
  avail.mo_i <- substr(mo_i, start =nchar(mo_i[1])-2, stop = nchar(mo_i[1])-1)
  
  avail.mo = rbind(avail.mo, c(i, find.months %in% avail.mo_i))
}

avail.mo <- as.data.frame(avail.mo)
colnames(avail.mo) = c("State", find.months)
DT::datatable(avail.mo) %>% formatStyle(2:13, background = styleEqual('TRUE', 'lightgreen'))
```

```{r, echo = F}
knitr::opts_chunk$set(echo = T, eval = F)
```

As a user in an RStudio session, this bucket may not be accessible using the aws.s3 package:
```{r s3connect_test, eval } 
library(aws.s3)

teambucket <- "s3://prod-sdc-sdi-911061262852-us-east-1-bucket"

get_bucket(teambucket)
get_location(teambucket)
bucket_exists(teambucket)

get_object()

```

The aws.s3 package in R looks for credentials in a `.aws` folder, which does not apply in this configuration (connection to S3 is based on SSH, per Santosh). So need to run the read/write scripts directly, either as shell scripts outside R or possibly passing AWS CLI commands via `system()` within R.


Possible solutions: 

From RStudio session, save files to a tempory output location, like "tempout"

1. Within the RStudio session, run the script `sdc_s3_out.sh`

```{r, eval = F}
output.loc <- "~/tempout"

# assuming a data.frame 'results' exists, from connect_redshift_pgsql.R
write.csv(results[1:100,], file = file.path(output.loc, "TestResults3.csv"), row.names = F)

dir("~/tempout") # list output files

system("~/SDI_Waze/utility/sdc_s3_out.sh")

```






2. As `ec2-user`, run the script `sdc_s3_out.sh`:
- From Jupyter Notebook, run the script `sdc_s3_out.sh` while logged in as `ec2-user`. This is the only user with permissions to write out to the shared S3 buckets.
  + Open a Juptyer Notebook session at https://<username>-workspace.securedatacommons.com:8888/
  + New > Terminal or Running > select an already-running terminal
  + Run the script as follows, with your username in the path:
  
```
sudo /home/daniel/SDI_Waze/utility/sdc_s3_out.sh
```

If this fails, make sure permissions are set up correctly, so that this `.sh` script can be executed by ec2-user:

```
sudo chmod +x /home/daniel/SDI_Waze/utility/sdc_s3_out.sh
```

Here is the script to write out some test files:

```{r}
output.loc <- "~/tempout"


# assuming a data.frame 'results' exists
write.csv(results[1:100,], file = file.path(output.loc, "TestResults3.csv"), row.names = F)
```

Then go to Juptyer Notebook terminal and run the script.


## Reading from shared bucket

From within RStudio session, get list of available objects and copy selected objects to this session.

Use a directory "tempin" to hold files copied from the S3 bucket. This should not be a permanent storage location for working files, just temporary directory to hold files during this session. 


```{r s3read}
dir.create("~/tempin", showWarnings = F) # will create it if does not already exist, will skip if already exists

list.cmd <- paste("aws s3 ls", teambucket)
system(list.cmd)

getobject.cmd <- paste("aws s3 cp", file.path(teambucket, "TestResults2.csv"), "~/tempin")

system(getobject.cmd)
```

Possible enhancement: make functions in `/utility` to carry out these list/read/write commands.


## Passing commands via `system`

This can allow more flexbilibty with specific variables, e.g. within ATA doing the following:

```
s3transfer = paste("aws s3 cp s3://ata-waze/MD_hexagon_shapefiles", localdir, "--recursive --include '*'")
  system(s3transfer)
```

For Waze_clip.R in SDC, can use the following to copy files from top level of team bucket to a subfolder:

```{r eval=F}
state = "MD"
dirtomake = paste0("Raw_", state)
s3transfer = paste0("aws s3 cp s3://prod-sdc-sdi-911061262852-us-east-1-bucket/ ", file.path("s3://prod-sdc-sdi-911061262852-us-east-1-bucket/", dirtomake), " --recursive --include '*.RData'")
system(s3transfer)
```

Get uploaded EDT data into the working data directory on the EC2 instance, after uploading via portal.securedatacommons.com:

```{r edtprep}
teambucket <- "s3://prod-sdc-sdi-911061262852-us-east-1-bucket/"
system(paste0("aws s3 ls ", teambucket))

# Get national hex and EDT new grid ID shapefiles:
system(paste0('aws s3 cp ', teambucket, ' ~/workingdata/ --include "shapefiles_*.zip"'))
# not working... do individually

system(paste0("aws s3 cp ", teambucket, "shapefiles_nationalHexagons.zip ~/workingdata/Hex"))
system(paste0("aws s3 cp ", teambucket, "shapefiles_newGRIDID.zip ~/workingdata/Hex"))
system(paste0("aws s3 cp ", teambucket, "ct_ut_va_hexagons_1mi.zip ~/workingdata/Hex"))

# 

# EDT data
system(paste0("aws s3 cp ", teambucket, "Maryland_april2017_to_present.csv ~/workingdata/EDT"))
system(paste0("aws s3 cp ", teambucket, "EDTsubset_april2017_to_present.zip ~/workingdata/EDT"))

```

Save output from EDT-Waze linking, from `~/workingdata/Link` to `Link` folder on teambucket:
```{r}
system(paste0("aws s3 cp ", "~/workingdata/Link/ ", teambucket, "Link/ --include '*' --recursive"))
```

### Deleting files:
Cannot delete fiels with `aws s3 rm`. Can only archive... to do

```{r}
# Can't pass wildcards still... copy and run in terminal
system(paste0("aws s3 cp ", teambucket, " ", teambucket, "Archive --include '*.csv'"))

paste0("aws s3 cp ", teambucket, " ", teambucket, "Archive --include '*.csv'")


```


### Moving files around in terminal

Example, after saving a bunch of files to `~/`, move all of them to a new directory within `~/workingdata` then save to S3 bucket:

```
mkdir ~/workingdata/State_accident_counts/
mv *_accidents_distinct.RData ~/workingdata/State_accident_counts/
aws s3 cp ~/workingdata/State_accident_counts/ s3://prod-sdc-sdi-911061262852-us-east-1-bucket/CountyCounts/ --recursive --include "*"
```


