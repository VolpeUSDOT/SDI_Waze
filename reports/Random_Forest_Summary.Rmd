---
title: ''
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

HEXSIZE = "1"

ONLOCAL = F

if(ONLOCAL){
  codeloc = "~/git/SDI_Waze"
  inputdir <- file.path("W:/SDI Pilot Projects/Waze/MASTER Data Files/Waze Aggregated/HexagonWazeEDT/WazeEDT Agg", HEXSIZE,"mile Rdata Input")
  outputdir1 <- file.path("//vntscex.local/DFS/Projects/PROJ-OR02A2/SDI/Model_Output/RF1")
  outputdir2 <- file.path("//vntscex.local/DFS/Projects/PROJ-OR02A2/SDI/Model_Output/RF2")

  localdir <- outputdir1 
  } else {
  codeloc <- "~/SDI_Waze" 
  inputdir <- paste0("WazeEDT_Agg", HEXSIZE, "mile_Rdata_Input")
  outputdir <- paste0("WazeEDT_Agg", HEXSIZE, "mile_RandForest_Output")
  aws.signature::use_credentials()
  waze.bucket <- "ata-waze"
  localdir <- "/home/dflynn-volpe/workingdata" 
}

library(tidyverse)
library(knitr)
library(kableExtra)
library(randomForest)
library(aws.s3)
library(pander)
library(DT)
# Run this if you don't have these packages:
# install.packages(c("kableExtra","randomForest","aws.s3","tidyverse","pander","DT"), dep = T)

knitr::opts_knit$set(root.dir = localdir)

# read utility functions
source(file.path(codeloc, 'utility/wazefunctions.R'))

# read random forest functions
source(file.path(codeloc, "analysis/RandomForest_WazeGrid_Fx.R"))

models <- read.csv(file.path(localdir, "Model_Descriptions.csv")) # Source: \\vntscex.local\DFS\Projects\PROJ-OR02A2\SDI\Model_Output

```

# Random Forest Overview

For the SDI Waze project, the analysis being used is a estimation of EDT crashes, based on Waze predictors. Currently, there are seven months of data where geolocated EDT and Waze data are available, from March - September, 2017, for Maryland. The models developed to date are training on data for which the presence of an EDT crash (binary) or count of EDT crashes is modeled based on a large number of predictors from the Waze data, such as number of Waze accidents, the type of Waze events, number of Waze events in total, and other variables. 

A random forest model trained on known data can then be employed on data for which only Waze data are provided. This would be the case if incoming Waze data is used to estimate the number and spatial/temporal pattern of EDT-level crashes for times or states when EDT data are not available. In the model development process, the testing is done for a subset of the data where the known EDT values are held back, and then the estimates produced by the model fit to training data can be compared to the known data. A goal of the SDI Waze project is to produce estimated EDT-level crash counts which best fit the observed data in terms of overall accuracy and precision in spatial and temporal patterns. 

The first phase of random forest models aimed to resolve initial questions about what features to include, as well as the consequence of adding months of data and the spatial grain of the data aggregation. The first phase used April, May, and June 2017 data from Maryland. Additional features beyond Waze data included hourly weather features, roadway characteristics, and socio-economic variables from census sources. 

The second phase of random forest models included a complete set of predictor features, including historical FARS accident counts, and average annual daily traffic (AADT). The second phase also extended the time extent of the training and testing across all available months of complete data, April - September 2017 for Maryland. 

There are multiple criteria for evaluating classification and regression models. For all the models, we used two different data sets to train and test the model. *Training* refers to fitting the model parameters with a large set of known EDT crashes and associated Waze events and other predictors, while *testing* refers to applying the fitted model parameters to a new set of Waze events and other predictors, generating estimated EDT crashes. The estimated EDT crashes are then compared to the known, observed EDT crashes in the test data set to evaluate model performance.

Here we use three criteria, each of which can also be applied to the regularized regression models in the next phase:

### 1. Diagnostics from a confusion matrix

  For binary classification models, it is possible to create a 2x2 table where columns are observed negative and positive, and rows are predicted negative and positive. This is know as a *confusion matrix*, and shows four quantities to represent model performance:

```{r diagtable, results = "asis"}
tabl <- "
 |          |        |       OBSERVED     |
 |----------|--------|:-------:|:--------:| 
 |          |        | Positive|  Negative|
 |<b>PREDICTED |Positive|   TP    |    FP |
 |          |Negative|   FN    |    TN    |
"
pander::pander(tabl, style = "rmarkdown")
```

False positives (FP) can be considered Type I errors, and false negatives (FN) can be considered Type II errors. 

  - *Accuracy* = TN + TP / All observations 
    + True positives and true negatives divided by all observations. A high value indicates that the observed occurrences and absences of EDT crashes are correctly being estimated. 
  
  - *Precision* = TP / FP + TP 
     + True positives divided by all predicted positives. A high value indicates that there are relatively few false positives (locations and times where a crash is estimated, but did not actually occur).
  
  - *Recall* = TP / FN + TP
    + True positives divided by all observed positives. This is also called *Sensitivity*, or the *true postitive rate*. A high value indicates that there are relatively few false negatives (locations and times where a crash was not estimated, but did actually occur).
  
  - *False Positive Rate* = FP / TN + FP
    + False positives divided by all observed negatives. A low value indicates that there are relatively few false positives compared to all observed absences of EDT crashes.


### 2. Mean Squared Error

Mean squared error is simply the mean squared difference between observed and estimated values (the residual error). This value is more relevant for models of continuous predictors (number of EDT crashes, rather than presence of EDT crashes).

### 3. Area under Reciever-Operator Characteristic Curve (AUROC, ROC curve)

For a binary model, we can additionally calculate one other quantity:

  - *Specificity* = TN / TN + FP
  + True negatives divided by all observed negatives, also called the true negative rate. For the SDI Waze analysis, most observations are "0", meaning no EDT crashes occurred, so much of the model performance is driven by accurately predicting these "0" (no crash) values.

Balancing between high specificity (where false positives are avoided) and high sensitivity (where false negatives are avoided) is an important decision point in evaluating a model. High sensitivity (i.e., recall or true positive rate) with high specificity (low false positive rate) is ideal. Plotting the false positive rate versus the true positive rate allows a visualization of this balance, and is known the 'receiver-operator characteristic (ROC) curve'. The larger the area under the ROC curve, the more high specificity is maximized with low loss of sensitivity. An area of 0.5 is equivalent to flipping a coin; and area of 1 is perfect estimation, with no false positives or false negatives. As a rule of thumb, areas of 0.6 or greater are generally considered to represent useful classification models; areas of 0.9 or greater are considered to represent very good classification models.


```{r roc, echo=FALSE, fig.cap="ROC curves, [CC BY-SA 3.0 Wikimedia](https://commons.wikimedia.org/w/index.php?curid=44059691)", out.width = '50%', , fig.align='center'}
knitr::include_graphics("ROC_curves.png")
```

# Phase I model testing

### Input data
In developing these models, it is important to keep in mind the goal is to estimate EDT-level crashes when and where no EDT data are present. For model development, two approaches to training and testing of the models can be used: split the observed data (where EDT data are available) into random subsets of training and test data, or develop models using all available data for one time period, and test the model on a new time period where EDT data are available. 

For Phase 1, we took the following approaches for Waze and EDT data from Maryland:

- April 2017 data, 70/30 split between training and test data
- April + May 2017, 70/30 split between training and test data
- April + May 2017, complete data used, test on June 2017

For most of the predictor combinations used, we focus on the first and third approaches. The first is a relatively small scale model, while the third is a more realistic model, where the model works to estimate EDT crashes for an entire month of data not used in the training set.       

### Spatial grain
We tested three options, and selected 1 mile grid size for subsequent work. Models at the larger spatial grain (4 square mile hexagonal grid cells) performed slightly worse than 1 mile grid cells, while the smaller spatial grain (0.5 square mile hexagonal grid cells) performed slightly better in most metrics. The small performance gain at the 0.5 mile grain size in some cases comes at the cost of substantially longer run time for data preparation (including approximately 36 hours of computational time per month of data to prepare weather variables at 0.5 mile grid size, compared to approximately 6 hours for a similar process for 1 mile grid cells), and 50% longer for model fitting, with 66,220 grid cells at 0.5 square miles versus 33,184 grid cells at 1 square miles. When training models over multiple states, for multiple months of data, such performance differences could be barriers to success. The subsequent models all use 1 square mile grid cells, but we will revisit the potential gain from a smaller spatial resolution.

Note that of the evaluation metrics, AUC was very high for nearly all models. This is driven by the high accuracy of these models, especially in estimating the true zeros (times and locations where no EDT crash occurred).

```{r phase1spatial, echo = F}
if(!exists("keyoutputs1")) {
  load(file.path(localdir, "Outputs_up_to_17"))
  keyoutputs1 <- keyoutputs
  rm(keyoutputs)
}

# pairs to compare: 01, 04, 06 - April
# 03, 05, 07 - April+May, test on June

tabl <- vector()
for(i in c("01", "04", "06")){ tabl <- rbind(tabl, c(100*keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))}

colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")

rownames(tabl) = c("01 - 1 mile    ", "04 - 4 mile    ", "06 - 0.5 mile    ")
#kable(tabl, caption = "Phase 1 model diagnostics for April 2017, Maryland.", rownames = "Model")
datatable(tabl, filter = 'top',
          caption = "Phase 1 model diagnostics for April 2017, Maryland.",
          rownames = T,
          options = list(dom = "t",
                         order = list(list(5, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))


tabl <- vector()
for(i in c("03", "05", "07")){ tabl <- rbind(tabl, c(100*keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))}

colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = c("03 - \n 1 mile   ", "05 - \n 4 mile  ", "07 - \n 0.5 mile  ")

datatable(tabl, filter = 'top',
          caption = "Phase 1 model diagnostics for April + May, test on June, Maryland.",
          rownames = T,
          options = list(dom = "t",
                         order = list(list(5, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))
```   

### Time windows
We considered, but deferred, analysis of different time windows. The models described here all use 1 hr aggregations of Waze event counts and EDT crashes. 4hr windows would require new aggregation process.

### Neighbors
One approach to addressing spatial dependence is to consider the counts of Waze events in neighboring grid cells as additional predictors of an EDT-level crash occurring.

Addition of neighbors drove minor increases in performance at both the one-month (April) and three month (April+May, test on June) data sets. Recall (minimization of false negatives) was higher when neighboring grid cells were included as predictors. Neighboring grid cells should be considered as useful additional predictors.

```{r phase1neighbors, echo = F}
# pairs to compare: 01, 08 - April
tabl <- vector()
for(i in c("01", "08")){ tabl <- rbind(tabl, c(100*keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))}

colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = c("01 - 1 mile", "08 - 1 mile, Neighbors")

datatable(tabl, filter = 'top',
          caption = "Phase 1 model diagnostics for April 2017, Maryland, with neighboring grid cells as predictors.",
          rownames = T,
          options = list(dom = "t",
                         order = list(list(5, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))

# 03, 09 - April+May, test on June
tabl <- vector()
for(i in c("03", "09")){ tabl <- rbind(tabl, c(100*keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))}
colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = c("03 - 1 mile", "09 - 1 mile, Neighbors")

datatable(tabl, filter = 'top',
          caption = "Phase 1 model diagnostics for April-May, test on June 2017, Maryland, with neighboring grid cells as predictors.",
          rownames = T,
          options = list(dom = "t",
                         order = list(list(5, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))
  
```   

### Additional data

Additional data included in Phase 1 were the following:

- Weather: Reflectivity from the [NEXRAD](https://www.ncdc.noaa.gov/data-access/radar-data/nexrad) radar network, pulled hourly and merged with gridded data.
- Road functional class: Miles of roads of each functional class, from [HPMS](https://www.fhwa.dot.gov/policyinformation/hpms.cfm).
- Economic data from [Longidtinal Employer-Household Dynamics](https://lehd.ces.census.gov/) data set: Number of jobs, and number of workers, for firms of different ages and sizes.

Adding these additional data increased the recall, overall accuracy, and AUC, at the cost of slightly higher false positive rates. Interestingly, the addition of weather mostly increased precision (minimizing false positives), while the sum of miles by road functional class and jobs data increased recall (minimizing false negatives).

```{r phase1addl, echo = F}
# pairs to compare: 12, 14, 16 - April
tabl <- vector()
for(i in c("08","12", "14", "16")){ tabl <- rbind(tabl, c(100*keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))}

colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = c("08 - 1 mile, Neighbors", "12 - Weather", "14 - Weather, Roads", "16 - Weather, Roads, Jobs")
datatable(tabl, filter = 'top',
          caption = "Phase 1 model diagnostics for April 2017, Maryland, with supplemental weather, road functional class, and economic data.",
          rownames = T,
          options = list(dom = "t",
                         order = list(list(5, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))

# 09, 13, 15, 17  - April+May, test on June
tabl <- vector()
for(i in c("09", "13", "15", "17")){ tabl <- rbind(tabl, c(100*keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4))) }
colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = c("09 - 1 mile, Neighbors", "13 - Weather", "15 - Weather, Roads", "17 - Weather, Roads, Jobs")
datatable(tabl, filter = 'top',
          caption = "Phase 1 model diagnostics for April + May, test on June, Maryland, with supplemental weather, road functional class, and economic data.",
          rownames = T,
          options = list(dom = "t",
                         order = list(list(5, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))
```   

The best model of this set, Model 17, had the following variables of greatest importance:

```{r bestphaseI, eval=T, fig.height=6, fig.width=6, fig.align='center'}
modelno = "17"

if(ONLOCAL) {load(file.path(outputdir1, paste("Model", modelno, "RandomForest_Output_04.RData", sep= "_")))} else {s3load(object = file.path(outputdir, paste("Model", modelno, "RandomForest_Output.RData", sep= "_")), bucket = waze.bucket)}

imp.tab <- data.frame(round(rf.out$importance[order(rf.out$importance, decreasing = T)[1:15],], 2))
colnames(imp.tab) = "Gini importance"

# kable(imp.tab, row.names = T,
#       caption = "Variance importance for the highest recall plot in phase I, Model 17. Top 15 of 74 predictors shown.") %>% 
#   kableExtra::kable_styling(bootstrap_options = "striped")

varImpPlot(rf.out,
           main = "Model 17 Variable Importance \n April+May, test on June",
           n.var = 20, 
           cex = 0.9)

```

## General summary 
What we learned from this testing is the following:

- All models showed very high accuracy and excellent performance according to ROC curves. 
- Even a single month of data presents a rich source of information for building an accurate EDT crash estimation model. For the Maryland data, going to longer time frames and testing on novel data (training on April + May, testing on June) showed similar overall model performance as the more simple single month of data (April only).
- 1 mi grid size is most practical, although there is some gain from the smaller grid size. That performance gain needs to be weighed against the higher computational resources needed for data aggregation and modeling.
- Neighboring grid cells added some additional predictive power.
- Additional data improved different aspects of the model: using the larger April + May data, weather data reduced false positives, while road functional class and jobs data reduced false negatives.





# Phase II model testing

In Phase II, we expand all models to all months of currently available EDT and Waze data, April to September 2017 for Maryland, to make more full use of the available data. We used a 70/30 split of training and test data across these months. 

In addition to the weather, road functional class, and jobs data used as supplemental data in Phase I, in Phase II we added the following supplemental data to attempt to further improve the precision and recall:

- Average annual daily traffic (AADT), by sum of the AADT in the roads in a grid cell, from HPMS
- Fatal Accident Reporting System (FARS) counts of fatal accidents from 2012-2016 for each grid cell

In addition, we considered correlated and nested variables, and devised a sequence of models to test to separate out the effects of Waze event types and sub-types. These models are organized in to "Sets" to facilitate comparison of how different combinations of predictors affect model performance.





```{r Diagframe2a}
if(ONLOCAL) {  localdir <- outputdir2 } 
load(file.path(localdir, "Output_to_44b"))
keyoutputs2 <- keyoutputs; rm(keyoutputs)
```


## A: All Waze 

Set A consisted of the following models:

- 18 Base: All Waze features from event type (but not the counts of all Waze events together)
- 19 Add FARS only
- 20 Add Weather only
- 21 Add road class, AADT only
- 22 Add jobs only
- 23 Add all together

Supplemental data increased recall and overall model fit. With more additional data, the false positive rate slightly increased, illustrating the trade-off between minimizing false negatives and minimizing false positives. For the purpose of maximizing the number of correct estimates of EDT-level crashes, a model with high recall would be preferred, Model 23 in this case.

The largest gain in predictive power came from adding AADT variables and road type.  


```{r phase2A, echo = F}
# pairs to compare: 18-23
tabl <- vector()
mods = as.character(18:23)
for(i in mods){ tabl <- rbind(tabl, c(100*keyoutputs2[[i]]$diag, round(keyoutputs2[[i]]$auc, 4)))}

colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = models[!is.na(match(models$Model.Number, mods)), "Short.Name"]
#kable(tabl, caption = "Phase II model diagnostics for April-September 2017, Maryland, with supplemental data added sequentually and together.", rownames = "Model")
datatable(tabl, filter = 'top',
          caption = "Phase II, Set A model diagnostics for April-September 2017, Maryland, with supplemental data added sequentually and together.",
          rownames = T,
          options = list(dom = "t",
                         order = list(list(5, 'desc')),
                         pageLength = length(mods))
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))
```

```{r bestphaseIIA, eval=T, fig.width=6,fig.height=6, fig.align='center'}
modelno = "33"

s3load(object = file.path(paste(outputdir, sep="_"), paste("Model", modelno, "RandomForest_Output.RData", sep= "_")), bucket = waze.bucket)

imp.tab <- data.frame(round(rf.out$importance[order(rf.out$importance, decreasing = T)[1:15],], 2))
colnames(imp.tab) = "Gini importance"
# 
# kable(imp.tab, row.names = T,
#       caption = "Variance importance for the highest recall plot in phase II, Model 33 Top 15 of 58 predictors shown") %>% 
#   kableExtra::kable_styling(bootstrap_options = "striped")

varImpPlot(rf.out, main = "Model 23 Variable Importance \n April-Sept", n.var = 15)

```

## B: TypeCounts

Set B consisted of the following models, focusing on Waze event types without the sub-types.

- 24 Base: nWazeAccident, nWazeJam, nWazeWeatherOrHazard, nWazeRoadClosed
- 25 Add other Waze only (confidence, reliability, magvar, neighbors)
- 26 Add FARS only
- 27 Add Weather only
- 28 Add road class, AADT only
- 29 Add jobs only
- 30 Add all (but not Waze event subtypes or sub-subtypes)
- 31 Test removing EDT only rows from model 30
- 32 Test removing road closure only rows from model 30

Excluding sub-types did not decrease overall accuracy and model fit, compared to Set A models. As with Set A, adding supplemental data beyond the base set of predictors increased recall, and also increased precision. The best balance between recall and precision, as measured by AUC, came again from the model with all supplemental predictors added. Road functional class and AADT again were the predictors which drove the largest increase in recall. All models exhibited excellent accuracy and overall performance.

```{r phase2B}
# pairs to compare: 24-32
tabl <- vector()
mods = as.character(24:32)
for(i in mods){ tabl <- rbind(tabl, c(100*keyoutputs2[[i]]$diag, round(keyoutputs2[[i]]$auc, 4)))}

colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = models[!is.na(match(models$Model.Number, mods)), "Short.Name"]

datatable(tabl, filter = 'top',
          caption = "Phase II, Set B model diagnostics for April-September 2017, Maryland, with supplemental data added sequentually and together.",
          rownames = T,
          options = list(dom = "t",
                         pageLength = length(mods))
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))
```

Approximately 1.5% of the data includes times and locations where only EDT values, but no Waze accidents, were present. These account for 30,875 of the 2,003,391 observations for the April-September data. Omitting these observations had a negligible effect on the overall model performance, with minor increase in recall and decrease in precision, leading to an overall minor decrease in AUC. 

An even smaller quantity of data included times and locations where only Waze road closure events were reported, 2,892 of the total observations (0.1%). Omitting these interestingly lead to the highest recall of this set of model, as well as the best balance between recall.

## C. SubtypeCounts

- 33 Base: nWazeAccidentMajor, nWazeAccidentMinor, nWazeJamModerate, nWazeJamHeavy, nWazeJamStandStill,
nHazardOnRoad, nHazardOnShoulder, nHazardWeather
- 34 Add other Waze only (confidence, reliability, magvar, neighbors)
- 35 Add FARS only
- 36 Add Weather only
- 37 Add road class, AADT only
- 38 Add jobs only
- 39 Add all (but not Waze event sub-subtypes)

Excluding Waze event types and only including sub-types drove a substantial degradation in model performance. Recall dropped substantially, and model fit was clearly lower than Set A or B models. Models should be constructed with either a combination of Waze types and sub-types, or just Waze event types.

```{r phase2C}
# pairs to compare: 33-39
tabl <- vector()
mods = as.character(33:39)
for(i in mods){ tabl <- rbind(tabl, c(100*keyoutputs2[[i]]$diag, round(keyoutputs2[[i]]$auc, 4)))}

colnames(tabl)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
rownames(tabl) = models[!is.na(match(models$Model.Number, mods)), "Short.Name"]
datatable(tabl, filter = 'top',
          caption = "Phase II, Set C model diagnostics for April-September 2017, Maryland, with supplemental data added sequentually and together.",
          rownames = T,
          options = list(dom = "t",
                         pageLength = length(mods))
          ) %>% formatCurrency(2:4, currency = "", digits = 2) %>%
          formatStyle(1, background = styleEqual(max(tabl[,1]), 'lightgreen'))%>%
          formatStyle(2, background = styleEqual(max(tabl[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(tabl[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(min(tabl[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(max(tabl[,5]), 'lightgreen'))
```


## D. EDT counts vs binary response:

- 42 Run best combination of each base model on counts vs binary 
- 43 Pick best and test removing EDT only rows
- 44 Pick best and test removing road closure only rows

Most grid cells have only 0 or 1 EDT crash. However, a small number of grid cells in time have more than 1 crash, less than 0.15% of the total data. Therefore, we also investigated the use of a continuous response variable (counts of EDT crashes) rather than a binary response variable (presence of EDT crashes). 

The continuous response version of the models can be compared by mean squared error (MSE), but not by the confusion matrix or AUC metrics, since it is not a classification approach. All of these are orders of magnitude smaller errors than the MSE for the binary classification approach, but caution needs to be used when comparing MSE for models using different data types. 

Model 42a, based on Model 23, provided the minimum MSE. This model includes all Waze event types and subtypes, as well as all supplemental data, without removing any rows of EDT-only or road closure-only data.

```{r, eval = F}
pctEDTcounts <- table(w.04_09$nMatchEDT_buffer_Acc) / nrow(w.04_09) *100
format(pctEDTcounts, scientific = F)
```

```{r phaseD}
# pairs to compare: 42a-44b
tabl <- vector()
mods = c("42a", "43a", "44a", "42b", "43b", "44b")

for(i in mods){ tabl <- rbind(tabl, c(keyoutputs2[[i]]$mse))}

colnames(tabl)[1] = c("MSE")
rownames(tabl) = models[!is.na(match(models$Model.Number, mods)), "Short.Name"]
datatable(tabl, filter = 'top',
          caption = "Phase II, Set D count model diagnostics for April-September 2017, Maryland.",
          rownames = T,
          options = list(dom = "t",
                         pageLength = length(mods))
          ) %>% formatSignif(1, digits = 4) %>%
          formatStyle(1, background = styleEqual(min(tabl[,1]), 'lightgreen'))

```

# Appendix

## Model definitions

The following models were tested:

```{r modeldefinitions}
models.use <- models[c("Set", "Model.Number","Short.Name", "Data","Additional.Data","Neighbors","MagVar..Rating")]


datatable(models.use,
          filter = 'top',
          caption = "Random forest model descriptions.",
          rownames = F,
          options = list(dom = "ftip",
                         pageLength = 10))
```

## Phase 1 Model peformance details

```{r keyoutputsummary1}
# Summary from keyoutputs1 ----
Nobs.frame <- vector()

for(i in 1:length(keyoutputs1)){
  Nobs.frame <- rbind(Nobs.frame, c(Model = names(keyoutputs1)[i], keyoutputs1[[i]][[1]]))
 }

datatable(Nobs.frame, filter = 'top',
          caption = "Phase 1 model inputs. N: total rows of input training data; No EDT = number rows without EDT crashes; EDT present: number of rows with EDT crashes; Waze accident present: Number of rows at least one `WazeAccident` present",
          rownames = T,
          options = list(dom = "tip",
                         #order = list(list(1, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:5, currency = "", digits = 0)

```

```{r Diagframe1}
tabl <- vector()

for(i in 1:length(keyoutputs1)){
  tabl <- rbind(tabl, c(Model = names(keyoutputs1)[i], 
                                    100*keyoutputs1[[i]][[3]],
                                    #Runtime = paste(round(keyoutputs1[[i]]$runtime, 1), attr(keyoutputs1[[i]]$runtime, "unit")),
                                    AUC = round(keyoutputs1[[i]]$auc, 4),
                                    MSE = round(keyoutputs1[[i]]$mse, 8)
                                    )
                      )
 }

colnames(tabl)[2:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate")

datatable(tabl, filter = 'top',
          caption = "Phase 1 model diagnostics. Best values are highlighed in green, worst in khaki.",
          rownames = T,
          options = list(dom = "tip",
                         order = list(list(1, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:5, currency = "", digits = 2) %>%
          formatStyle(2, background = styleEqual(range(tabl[,2]), c('khaki','lightgreen')))%>%
          formatStyle(3, background = styleEqual(range(tabl[,3]), c('khaki','lightgreen')))%>%
          formatStyle(4, background = styleEqual(range(tabl[,4]), c('khaki','lightgreen')))%>%
          formatStyle(5, background = styleEqual(range(tabl[,5]), c('lightgreen','khaki')))%>%
          formatStyle(6, background = styleEqual(range(tabl[,6]), c('khaki','lightgreen')))%>%
          formatStyle(7, background = styleEqual(range(tabl[,7]), c('lightgreen','khaki')))

```

## Phase 2 Model peformance details


```{r keyoutputsummary2}
Nobs.frame <- vector()

for(i in 1:length(keyoutputs2)){
  Nobs.frame <- rbind(Nobs.frame, c(Model = names(keyoutputs2)[i], keyoutputs2[[i]][[1]]))
 }

datatable(Nobs.frame, filter = 'top',
          caption = "Phase 2 model inputs. N: total rows of input training data; No EDT = number rows without EDT crashes; EDT present: number of rows with EDT crashes; Waze accident present: Number of rows at least one `WazeAccident` present",
          rownames = T,
          options = list(dom = "tip",
                         pageLength = 5)
          ) %>% formatCurrency(2:5, currency = "", digits = 0)

```

```{r Diagframe2}
continuousmods <- c("42a", "42b", "43a", "43b", "44a","44b")

k2 <- keyoutputs2[is.na(match(names(keyoutputs2), continuousmods))]

tabl <- vector()
for(i in 1:length(k2)){
  
  tabl <- rbind(tabl, c(Model = names(k2)[i], 
                                    100*k2[[i]][[3]],
                                    #Runtime = paste(round(k2[[i]]$runtime, 1), attr(k2[[i]]$runtime, "unit")),
                                    AUC = round(k2[[i]]$auc, 4),
                                    MSE = round(k2[[i]]$mse, 8)
                                    )
                      )
 }
# 
# for(i in continuousmods){
#   
#   tabl <- rbind(tabl, c(Model = i, rep(NA, 5), MSE = round(keyoutputs2[[i]]$mse, 10)
#                                     )
#                       )
#  }


colnames(tabl)[2:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate")

datatable(tabl, filter = 'top',
          caption = "Phase 2 model diagnostics. Best values are highlighed in green, worst in khaki.",
          rownames = T,
          options = list(dom = "tip",
                         order = list(list(1, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:5, currency = "", digits = 2) %>%
          formatStyle(2, background = styleEqual(range(tabl[,2]), c('khaki','lightgreen')))%>%
          formatStyle(3, background = styleEqual(range(tabl[,3]), c('khaki','lightgreen')))%>%
          formatStyle(4, background = styleEqual(range(tabl[,4]), c('khaki','lightgreen')))%>%
          formatStyle(5, background = styleEqual(range(tabl[,5]), c('lightgreen','khaki')))%>%
          formatStyle(6, background = styleEqual(range(tabl[,6]), c('khaki','lightgreen')))%>%
          formatStyle(7, background = styleEqual(range(tabl[,7]), c('lightgreen','khaki')))
```
