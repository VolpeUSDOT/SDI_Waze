---
title: ""
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
  html_notebook:
    df_print: paged
    self_contained: yes
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

HEXSIZE = "1"

ONLOCAL = F

if(ONLOCAL){
  codeloc = "~/git/SDI_Waze"
  inputdir <- file.path("W:/SDI Pilot Projects/Waze/MASTER Data Files/Waze Aggregated/HexagonWazeEDT/WazeEDT Agg", HEXSIZE,"mile Rdata Input")
  outputdir1 <- file.path("//vntscex.local/DFS/Projects/PROJ-OR02A2/SDI/Model_Output/RF1")
  outputdir2 <- file.path("//vntscex.local/DFS/Projects/PROJ-OR02A2/SDI/Model_Output/RF2")

  localdir <- outputdir1 
  } else {
  codeloc <- "~/SDI_Waze" 
  inputdir <- paste0("WazeEDT_Agg", HEXSIZE, "mile_Rdata_Input")
  outputdir <- paste0("WazeEDT_Agg", HEXSIZE, "mile_RandForest_Output")
  aws.signature::use_credentials()
  waze.bucket <- "ata-waze"
  localdir <- "/home/dflynn-volpe/workingdata" 
}

library(knitr)
library(kableExtra)
library(randomForest)
library(aws.s3)
library(tidyverse)
library(pander)
library(DT)
# Run this if you don't have these packages:
# install.packages(c("kableExtra","randomForest","aws.s3","tidyverse","pander","DT"), dep = T)

knitr::opts_knit$set(root.dir = localdir)

# read utility functions
source(file.path(codeloc, 'utility/wazefunctions.R'))

# read random forest functions
source(file.path(codeloc, "analysis/RandomForest_WazeGrid_Fx.R"))

models <- read.csv(file.path(localdir, "Model_Descriptions.csv")) # Source: \\vntscex.local\DFS\Projects\PROJ-OR02A2\SDI\Model_Output

```

# Random Forest Overview

For the SDI Waze project, the analysis being used is a estimation of EDT crashes, based on Waze predictors. Currently, there are seven months of data where geolocated EDT and Waze data are available, from March - September, 2017, for Maryland. The models developed to date are training on data for which the presence of an EDT crash (binary) or count of EDT crashes is modeled based on a large number of predictors from the Waze data, such as number of Waze accidents, the type of Waze events, number of Waze events in total, and other variables. 

A random forest model trained on known data can then be employed on data for which only Waze data are provided. This would be the case if incoming Waze data is used to estimate the number and spatial/temporal pattern of EDT-level crashes for times or states when EDT data are not available. In the model development process, the testing is done for a subset of the data where the known EDT values are held back, and then the estimates produced by the model fit to training data can be compared to the known data. A goal of the SDI Waze project is to produce estimated EDT-level crash counts which best fit the observed data in terms of overall accuracy and precision in spatial and temporal patterns. 

The first phase of random forest models aimed to resolve initial questions about what features to include, as well as the consequence of adding months of data and the spatial grain of the data aggregation. The first phase used April, May, and June 2017 data from Maryland. Additional features beyond Waze data included hourly weather features, roadway characteristics, and socio-economic variables from census sources. 

The second phase of random forest models included a complete set of predictor features, including historical FARS accident counts, and average annual daily traffic (AADT). The second phase also extended the time extent of the training and testing across all available months of complete data, April - September 2017 for Maryland. 

There are multiple criteria for evaluating classification and regression models. For all the models, we used two different data sets to train and test the model. *Training* refers to fitting the model parameters with a large set of known EDT crashes and associated Waze events and other predictors, while *testing* refers to applying the fitted model parameters to a new set of Waze events and other predictors, generating estimated EDT crashes. The estimated EDT crashes are then compared to the known, observed EDT crashes in the test data set to evaluate model performance.

Here we use three criteria, each of which can also be applied to the regularized regression models in the next phase:

### 1. Diagnostics from a confusion matrix

  For binary classification models, it is possible to create a 2x2 table where columns are observed negative and positive, and rows are predicted negative and positive. This is know as a *confusion matrix*, and shows four quantities to represent model performance:

```{r diagtable, results = "asis"}
tabl <- "
 |          |        |       OBSERVED     |
 |----------|--------|:-------:|:--------:| 
 |          |        | Positive|  Negative|
 |<b>PREDICTED |Positive|   TP    |    FP |
 |          |Negative|   FN    |    TN    |
"
pander::pander(tabl, style = "rmarkdown")
```

False positives (FP) can be considered Type I errors, and false negatives (FN) can be considered Type II errors. 

  - *Accuracy* = TN + TP / All observations 
    + True positives and true negatives divided by all observations. A high value indicates that the observed occurrences and absences of EDT crashes are correctly being estimated. 
  
  - *Precision* = TP / FP + TP 
     + True positives divided by all predicted positives. A high value indicates that there are relatively few false positives (locations and times where a crash is estimated, but did not actually occur).
  
  - *Recall* = TP / FN + TP
    + True positives divided by all observed positives. This is also called *Sensitivity*, or the *true postitive rate*. A high value indicates that there are relatively few false negatives (locations and times where a crash was not estimated, but did actually occur).
  
  - *False Positive Rate* = FP / TN + FP
    + False positives divided by all observed negatives. A low value indicates that there are relatively few false positives compared to all observed absences of EDT crashes.


### 2. Mean Squared Error

Mean squared error is simply the mean squared difference between observed and estimated values (the residual error). This value is more relevant for models of continuous predictors (number of EDT crashes, rather than presence of EDT crashes).

### 3. Area under Reciever-Operator Characteristic Curve (AUROC, ROC curve)

For a binary model, we can additionally calculate one other quantity:

  - *Specificity* = TN / TN + FP
  + True negatives divided by all observed negatives, also called the true negative rate. For the SDI Waze analysis, most observations are "0", meaning no EDT crashes occurred, so much of the model performance is driven by accurately predicting these "0" (no crash) values.

Balancing between high specificity (where false positives are avoided) and high sensitivity (where false negatives are avoided) is an important decision point in evaluating a model. High sensitivity (i.e., recall or true positive rate) with high specificity (low false positive rate) is ideal. Plotting the false positive rate versus the true positive rate allows a visualization of this balance, and is known the 'receiver-operator characteristic (ROC) curve'. The larger the area under the ROC curve, the more high specificity is maximized with low loss of sensitivity. An area of 0.5 is equivalent to flipping a coin; and area of 1 is perfect estimation, with no false positives or false negatives. As a rule of thumb, areas of 0.6 or greater are generally considered to represent useful classification models; areas of 0.9 or greater are considered to represent very good classification models.


```{r roc, echo=FALSE, fig.cap="ROC curves, [CC BY-SA 3.0 Wikimedia](https://commons.wikimedia.org/w/index.php?curid=44059691)", out.width = '50%'}
knitr::include_graphics("ROC_curves.png")
```

# Phase 1 model testing

### Input data
In developing these models, it is important to keep in mind the goal is to estimate EDT-level crashes when and where no EDT data are present. For model development, two approaches to training and testing of the models can be used: split the observed data (where EDT data are available) into random subsets of training and test data, or develop models using all available data for one time period, and test the model on a new time period where EDT data are available. 

For Phase 1, we took the following approaches for Waze and EDT data from Maryland:

- April 2017 data, 70/30 split between training and test data
- April + May 2017, 70/30 split between training and test data
- April + May 2017, complete data used, test on June 2017

For most of the predictor combinations used, we focus on the first and third approaches. The first is a relatively small scale model, while the third is a more realistic model, where the model works to estimate EDT crashes for an entire month of data not used in the training set.       

### Spatial grain
We tested three options, and selected 1 mile grid size for subsequent work. Models at the larger spatial grain (4 square mile hexagonal grid cells) performed slightly worse than 1 mile grid cells, while the smaller spatial grain (0.5 square mile hexagonal grid cells) performed slightly better in most metrics. The small performance gain at the 0.5 mile grain size in some cases comes at the cost of substantially longer run time for data preparation (including approximately 36 hours of computational time per month of data to prepare weather variables at 0.5 mile grid size, compared to approximately 6 hours for a similar process for 1 mile grid cells), and 50% longer for model fitting, with 66,220 grid cells at 0.5 square miles versus 33,184 grid cells at 1 square miles. When training models over multiple states, for multiple months of data, such performance differences could be barriers to success. The subsequent models all use 1 square mile grid cells, but we will revisit the potential gain from a smaller spatial resolution.

Note that of the evaluation metrics, AUC was very high for nearly all models. This is driven by the high accuracy of these models, especially in estimating the true zeros (times and locations where no EDT crash occurred).

```{r phase1spatial, echo = F}
if(!exists("keyoutputs1")) {
  load(file.path(localdir, "Outputs_up_to_17"))
  keyoutputs1 <- keyoutputs
  rm(keyoutputs)
  # simplifying the auc value, don't need complete output
  # for(i in 1:length(keyoutputs)){
  #   keyoutputs[[i]]$auc <- as.numeric(keyoutputs[[i]]$auc)
  # }
  # save(list = "keyoutputs", file = file.path(localdir, "Outputs_up_to_17"))
}

# pairs to compare: 01, 04, 06 - April
# 03, 05, 07 - April+May, test on June

Diag.frame <- vector()
for(i in c("01", "04", "06")){
  Diag.frame <- rbind(Diag.frame, c(keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))
 }

colnames(Diag.frame)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
Diag.frame[,1:4] = Diag.frame[,1:4]*100

rownames(Diag.frame) = c("Model 01 - 1 mile", "Model 04 - 4 mile", "Model 06 - 0.5 mile")
kable(Diag.frame, 
          caption = "Phase 1 model diagnostics for April 2017, Maryland.",
          rownames = "Model")


Diag.frame <- vector()
for(i in c("03", "05", "07")){
  Diag.frame <- rbind(Diag.frame, c(keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))
 }

colnames(Diag.frame)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
Diag.frame[,1:4] = Diag.frame[,1:4]*100

rownames(Diag.frame) = c("Model 03 - 1 mile", "Model 05 - 4 mile", "Model 07 - 0.5 mile")
kable(Diag.frame, 
          caption = "Phase 1 model diagnostics for April + May, test on June, Maryland.",
          rownames = "Model")

```   

### Time windows
We considered, but deferred, analysis of different time windows. The models described here all use 1 hr aggregations of Waze event counts and EDT crashes. 4hr windows would require new aggregation process.

### Neighbors
One approach to addressing spatial dependence is to consider the counts of Waze events in neighboring grid cells as additional predictors of an EDT-level crash occurring.

Addition of neighbors drove minor increases in performance at both the one-month (April) and three month (April+May, test on June) data sets. Recall (minimization of false negatives) was higher when neighboring grid cells were included as predictors. Neighboring grid cells should be considered as useful additional predictors.

```{r phase1neighbors, echo = F}
# pairs to compare: 01, 08 - April
Diag.frame <- vector()
for(i in c("01", "08")){
  Diag.frame <- rbind(Diag.frame, c(keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))
 }

colnames(Diag.frame)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
Diag.frame[,1:4] = Diag.frame[,1:4]*100
rownames(Diag.frame) = c("Model 01 - 1 mile", "Model 08 - 1 mile, Neighbors")
kable(Diag.frame, 
          caption = "Phase 1 model diagnostics for April 2017, Maryland, with neighboring grid cells as predictors.",
          rownames = "Model")

# 03, 09 - April+May, test on June
Diag.frame <- vector()
for(i in c("03", "09")){
  Diag.frame <- rbind(Diag.frame, c(keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))
 }
colnames(Diag.frame)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
Diag.frame[,1:4] = Diag.frame[,1:4]*100
rownames(Diag.frame) = c("Model 03 - 1 mile", "Model 09 - 1 mile, Neighbors")
kable(Diag.frame, 
          caption = "Phase 1 model diagnostics for April + May, test on June, Maryland.",
          rownames = "Model")
```   

### Additional data

Additional data included in Phase 1 were the following:

- Weather
- Road functional class
- Economic data from LODES

Adding these additional data increased the recall, overall accuracy, and AUC, at the cost of slightly higher false positive rates. Interestingly, the addition of weather appeared to mostly increase precision (minimizing false positives).

```{r phase1addl, echo = F}
# pairs to compare: 12, 14, 16 - April
Diag.frame <- vector()
for(i in c("08","12", "14", "16")){
  Diag.frame <- rbind(Diag.frame, c(keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))
 }

colnames(Diag.frame)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
Diag.frame[,1:4] = Diag.frame[,1:4]*100
rownames(Diag.frame) = c("Model 08 - 1 mile, Neighbors", "Model 12 - Weather", "Model 14 - Weather, Roads", "Model 16 - Weather, Roads, Jobs")
kable(Diag.frame, 
          caption = "Phase 1 model diagnostics for April 2017, Maryland, with neighboring grid cells as predictors.",
          rownames = "Model")

# 09, 13, 15, 17  - April+May, test on June
Diag.frame <- vector()
for(i in c("09", "13", "15", "17")){
  Diag.frame <- rbind(Diag.frame, c(keyoutputs1[[i]]$diag, round(keyoutputs1[[i]]$auc, 4)))
 }
colnames(Diag.frame)[1:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate", "AUC")
Diag.frame[,1:4] = Diag.frame[,1:4]*100
rownames(Diag.frame) =c("Model 09 - 1 mile, Neighbors", "Model 13 - Weather", "Model 15 - Weather, Roads", "Model 17 - Weather, Roads, Jobs")
kable(Diag.frame, 
          caption = "Phase 1 model diagnostics for April + May, test on June, Maryland.",
          rownames = "Model")
```   


## General summary 
What we learned from this testing is the following:

- 1mi grid size is best. There was 

- Adding more months of data does not really improve model: focus on adding data features
- Run future models on April-Sept (6 months) with 70/30 training/test split

Some improvements with some new data features. Expand testing in next phase
Something about neighbors?

```{r keyoutputs1ummary1}
# Summary from keyoutputs1 ----
Nobs.frame <- vector()

for(i in 1:length(keyoutputs1)){
  Nobs.frame <- rbind(Nobs.frame, c(Model = i, keyoutputs1[[i]][[1]]))
 }

datatable(Nobs.frame, filter = 'top',
          caption = "Phase 1 model inputs. N: total rows of input training data; No EDT = number rows without EDT crashes; EDT present: number of rows with EDT crashes; Waze accident present: Number of rows at least one `WazeAccident` present",
          rownames = T,
          options = list(dom = "tip",
                         #order = list(list(1, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:5, currency = "", digits = 0)

```

```{r Diagframe1}
Diag.frame <- vector()

for(i in 1:length(keyoutputs1)){
  Diag.frame <- rbind(Diag.frame, c(Model = i, 
                                    100*keyoutputs1[[i]][[3]],
                                    Runtime = paste(round(keyoutputs1[[i]]$runtime, 1), attr(keyoutputs1[[i]]$runtime, "unit")),
                                    AUC = round(keyoutputs1[[i]]$auc, 4),
                                    MSE = round(keyoutputs1[[i]]$mse, 8)
                                    )
                      )
 }

colnames(Diag.frame)[2:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate")
Diag.frame[,1] = as.factor(Diag.frame[,1])

datatable(Diag.frame, filter = 'top',
          caption = "Phase 1 model diagnostics. Best values are highlighed in green.",
          rownames = T,
          options = list(dom = "tip",
                         order = list(list(1, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:5, currency = "", digits = 2) %>%
          formatStyle(2, background = styleEqual(max(Diag.frame[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(Diag.frame[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(max(Diag.frame[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(min(Diag.frame[,5]), 'lightgreen'))%>%
          formatStyle(7, background = styleEqual(max(Diag.frame[,7]), 'lightgreen'))%>%
          formatStyle(8, background = styleEqual(min(Diag.frame[,8]), 'lightgreen'))
```

```{r m01-03, eval=F}
modelno = "16"

if(ONLOCAL) {load(file.path(outputdir1, paste("Model", modelno, "RandomForest_Output_04.RData", sep= "_")))} else {s3load(object = file.path(outputdir, paste("Model", modelno, "RandomForest_Output.RData", sep= "_")), bucket = waze.bucket)}


rf.04$call
kable(round(rf.04$importance[order(rf.04$importance, decreasing = T),], 3))


modelno = "02"

modelno = "03"


```



# Phase 2 model testing
- Expand all models to six months (April to September 70/30 training test split)
- Start with base models below (A then B then C)
- Run and document tests in markdown file and run with updated RF functions


```{r Diagframe2}
if(ONLOCAL) {  localdir <- outputdir2 } 

if(!exists("keyoutputs2")) { 
  load(file.path(localdir, "Output_to_45a"))
  keyoutputs2 <- keyoutputs
  rm(keyoutputs) 
#   simplifying the auc value, don't need complete output
#   for(i in 1:length(keyoutputs)){
#     keyoutputs[[i]]$auc <- as.numeric(keyoutputs[[i]]$auc)
#   }
#   save(list = "keyoutputs", file = file.path(localdir, "Output_to_45a"))
  }

Diag.frame <- vector()

for(i in 1:length(keyoutputs2)){
  Diag.frame <- rbind(Diag.frame, c(Model = names(keyoutputs2)[i], 
                                    100*keyoutputs2[[i]][[3]],
                                    Runtime = paste(round(keyoutputs2[[i]]$runtime, 1), attr(keyoutputs2[[i]]$runtime, "unit")),
                                    AUC = round(keyoutputs2[[i]]$auc, 4),
                                    MSE = round(keyoutputs2[[i]]$mse, 8)
                                    )
                      )
 }

colnames(Diag.frame)[2:5] = c("Accuracy", "Precision", "Recall", "False Positive Rate")
#Diag.frame[,1] = as.factor(Diag.frame[,1])

datatable(Diag.frame, filter = 'top',
          caption = "Phase 2 model diagnostics. Best values are highlighed in green.",
          rownames = T,
          options = list(dom = "tip",
                         order = list(list(1, 'desc')),
                         pageLength = 5)
          ) %>% formatCurrency(2:5, currency = "", digits = 2) %>%
          formatStyle(2, background = styleEqual(max(Diag.frame[,2]), 'lightgreen'))%>%
          formatStyle(3, background = styleEqual(max(Diag.frame[,3]), 'lightgreen'))%>%
          formatStyle(4, background = styleEqual(max(Diag.frame[,4]), 'lightgreen'))%>%
          formatStyle(5, background = styleEqual(min(Diag.frame[,5]), 'lightgreen'))%>%
          formatStyle(7, background = styleEqual(max(Diag.frame[,7]), 'lightgreen'))%>%
          formatStyle(8, background = styleEqual(min(Diag.frame[,8]), 'lightgreen'))
```


## Set A: All Waze 

- 18 Base: All Waze features from event type (but not the counts of all Waze events together)
- 19 Add FARS only
- 20 Add Weather only
- 21 Add road class, AADT only
- 22 Add jobs only
- 23 Add all together

Identify best combination and explore/visualize/document performance

```{r m18, eval = F}
modelno = "18"

s3load(object = file.path(outputdir, paste("Model", modelno, "RandomForest_Output.RData", sep= "_")), 
       bucket = waze.bucket)

rf.out$call
kable(round(rf.out$importance[order(rf.out$importance, decreasing = T),], 3))

```

## B: TypeCounts

- 24 Base: nWazeAccident, nWazeJam, nWazeWeatherOrHazard, nWazeRoadClosed
- 25 Add other Waze only (confidence, reliability, magvar, neighbors)
- 26 Add FARS only
- 27 Add Weather only
- 28 Add road class, AADT only
- 29 Add jobs only
- 30 Add all (but not Waze event subtypes or sub-subtypes)
- 31 Pick best and test removing EDT only rows
- 32 Pick best and test removing road closure only rows

```{r m24}

```

```{r m25}

```


## C. SubtypeCounts

- 33 Base: nWazeAccidentMajor, nWazeAccidentMinor, nWazeJamModerate, nWazeJamHeavy, nWazeJamStandStill,
nHazardOnRoad, nHazardOnShoulder, nHazardWeather
- 34 Add other Waze only (confidence, reliability, magvar, neighbors)
- 35 Add FARS only
- 36 Add Weather only
- 37 Add road class, AADT only
- 38 Add jobs only
- 39 Add all (but not Waze event sub-subtypes)
- 40 Pick best and test removing EDT only rows
- 41 Pick best and test removing road closure only rows

```{r m33}

```

```{r m34}
```



## D. EDT counts vs binary response:

- 42 Run best combination of each base model (A, B, and C) on counts vs binary 
- 43 Pick best and test removing EDT only rows
- 44 Pick best and test removing road closure only rows

## E. Buffer vs grid cell counts for response variable

- 45 Run best combination of each base model (A, B, and C) on buffer counts vs grid cell counts 
- 46 Pick best and test removing EDT only rows
- 47 Pick best and test removing road closure only rows

# Appendix: Model definitions

The following models were tested:

```{r modeldefinitions}
models.use <- models[c("Set", "Model.Number","Short.Name", "Data","Additional.Data","Neighbors","MagVar..Rating")]


datatable(models.use,
          filter = 'top',
          caption = "Random forest model descriptions.",
          rownames = F,
          options = list(dom = "ftip",
                         pageLength = 10))
```