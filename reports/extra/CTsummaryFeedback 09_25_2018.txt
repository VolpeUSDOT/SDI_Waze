The Safety Data Initiative Waze project aims to estimate the patterns and processes associated with roadway crashes, based on crowdsourced Waze predictors. The first phase of the effort in early 2018 focused on data available at that time. Maryland contributes crash data to the Electronic Data Transfer (EDT) system, with high quality crash location data available for most records. Crash estimation models (random forests) were trained on the presence or absence of EDT crashes (binary), in each 1 mile grid cell, for each hour of the study period with reported Waze events. Combinations of a large number of predictors such as number of Waze accidents, the type of Waze events, number of Waze events in total, and other auxiliary variables were tested to identify the models that best estimated the presence and absence of EDT crashes.
A model trained on observed EDT crash data can be employed to estimate crash counts using Waze input variables to estimate the number and spatial/temporal pattern of EDT-level crashes for times or states when EDT data are not available. In the model development process, the model is first trained on a majority (~70%) of the complete data. The remainder of the complete data is used to assess the model performance; the estimates produced by the model fit to training data can be compared to the observed EDT data. A goal of the SDI Waze project is to produce estimated EDT-level crash counts which best fit the observed data in terms of overall accuracy and precision in spatial and temporal patterns.
In the first phase of the SDI Waze project, the approach was tested for six-months of Waze and EDT data in Maryland (April - September, 2017) to assess the fitness of the Waze data for estimating police-reportable crashes. The models showed high accuracy, with over 99% of locations and times being correctly identified has having a police-reportable crash or not. In addition to the Waze event reports, a number of supplemental data sources were found to improve recall and accuracy. However, overall accuracy was already high without these supplemental features. The supplemental data sources included the following:
* Average annual daily traffic (AADT), summed for all the roads in a grid cell, from HPMS.
* Fatal Accident Reporting System (FARS) counts of total fatal accidents from 2012-2016 for each grid cell.
* Weather: Reflectivity from the NEXRAD radar network, pulled hourly and merged with gridded data.
* Road functional class: Miles of roads of each functional class, from HPMS.
* Economic data from Longitudinal Employer-Household Dynamics (LEHD) data set: Number of jobs, and number of workers, for firms of different ages and sizes.
Application to Connecticut
To extend the work of the first phase to additional states, Connecticut was selected in the second phase of the SDI Waze project. Like Maryland, crashes are reported in the EDT system for Connecticut, and spatial data are high quality with high coverage across crashes (i.e., relatively few crashes have no spatial data). Two sets of models were applied, a simple model using only Waze variables (32 different variables), and a full model with an additional 26 variables from AADT, vehicle miles traveled (VMT) by road functional class, FARS, and LEHD economic variables.
As a test of the approach taken in the first phase, a model of police-reportable crashes using just Waze data features (without supplemental data) was applied to Connecticut, for the same April - September 2017 time period (the simple model, below). We then compare this Connecticut model to the Maryland model, and repeat the test for the full model (with supplemental data). There are three questions to ask in this comparison:
1. Do the Connecticut and Maryland models have similar accuracy, precision, recall, and false positive rates?
2. Do the Connecticut and Maryland models have similar variables identified as the most important?
3. When applying models fitted to one state on to data from another state, how much does the accuracy and other model diagnostic information change?
For both Maryland and Connecticut, there are two key data sets:
* EDT data, April - September 2017
* Waze data, April - September 2017
For both simple and full random forest models, models were fitted using 70% of the data (by grid cell and hour). Then, again within each state, 30% of data was used to test the fitted model. Observed EDT crashes are compared to estimated EDT crashes produced by the model to assess model performance.

Question 1: Do the Connecticut and Maryland models have similar accuracy, precision, recall, and false positive rates?
Models for both states show high accuracy; see Appendix below for model diagnostic definitions. The Connecticut model shows higher precision and recall; overall, AUC is high for both state models, but slightly higher for Connecticut (0.98 versus 0.96 for Maryland). Including the full set of variables increased accuracy, recall, and AUC in both states.

In the simple model, the variable with the most explanatory power, nWazeAccident, accounted for 60.5% and 56.8% of the variation in estimated EDT crashes [SE(1]in the Connecticut and Maryland models, respectively. The top six variables are the same for both states, and the importance values for these variables are nearly the same. Besides the Waze accident variables, the amount of freeways (road type 3, nWazeRT3), hour of day, and amount of primary roads (road type 6, nWazeRT6) were the most important variables, in the same order, for both states.

Question 3. When applying models fitted to one state to data from another state, how does model performance change?

To answer this question, we used models trained on one state (e.g., Maryland) and applied it to the full set of data from the other state (e.g., Connecticut). This process would be similar to what would be done if applying this approach to estimating police-reportable crashes for states where EDT data are not available.
The performance of models trained for one state and applied to the other was high; using accuracy and AUC metrics, there was no decrease in performance at all, but in fact some increase in AUC. Since both models had highly similar importance of individual variables (the top six variables being identical in both models), it is not surprising that the performance is similar. These results demonstrate that the relationship between the crowdsourced Waze data and police-reportable crashes is consistent and can be used with confidence when EDT data are not available[SE(2].
Interestingly, the Maryland model applied to Connecticut data performed very well, with higher recall and precision than the Connecticut model. However, using the Connecticut model on Maryland data did show trade-offs between lower precision and higher recall, which is also reflected in the higher false positive rates in both the simple and full models. This indicates that the Connecticut models are slightly more ‘generous’ than the Maryland model; the Connecticut models are more likely to determine that a police-reportable crash has occurred in a given grid cell in an hour, given the combination of Waze predictor variables used.



[SE(1]Is this accurate in the RF framework? This sounds like regression… 
[SE(2]Qualify this a bit? MD and CT are prob more similar than MD and UT?
